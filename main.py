import os
import cv2
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import pytorch_lightning as pl
from torch import nn
from tqdm.notebook import tqdm

IMAGE_SIZE = 320
BATCH_SIZE = 16
EPOCHS = 5

color_dict = pd.read_csv(
    "C:/Users/sande/OneDrive/Documents/minor-project-2/class_dict.csv"
)
CLASSES = color_dict["name"]
print(color_dict)
from glob import glob
from sklearn.utils import shuffle

pd_dataset = pd.DataFrame(
    {
        "IMAGES": sorted(
            glob("C:/Users/sande/OneDrive/Documents/minor-project-2/train/*.jpg")
        ),
        "MASKS": sorted(
            glob("C:/Users/sande/OneDrive/Documents/minor-project-2/train/*.png")
        ),
    }
)
pd_dataset = pd_dataset.sample(frac=1).reset_index(drop=True)
pd_dataset.head()
print(pd_dataset)
from sklearn.model_selection import train_test_split

pd_train, pd_test = train_test_split(pd_dataset, test_size=0.25, random_state=0)
pd_train, pd_val = train_test_split(pd_train, test_size=0.2, random_state=0)

print("Training set size:", len(pd_train))
print("Validation set size:", len(pd_val))
print("Testing set size:", len(pd_test))
for index in range(5):

    sample_img = cv2.imread(pd_train.iloc[index].IMAGES)
    sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)

    sample_msk = cv2.imread(pd_train.iloc[index].MASKS)
    sample_msk = cv2.cvtColor(sample_msk, cv2.COLOR_BGR2RGB)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))

    ax1.set_title("IMAGE")
    ax1.imshow(sample_img)

    ax2.set_title("MASK")
    ax2.imshow(sample_msk)


def rgb2category(rgb_mask):
    category_mask = np.zeros(rgb_mask.shape[:2], dtype=np.int8)
    for i, row in color_dict.iterrows():
        category_mask += (
            np.all(
                rgb_mask.reshape((-1, 3)) == (row["r"], row["g"], row["b"]), axis=1
            ).reshape(rgb_mask.shape[:2])
            * i
        )
    return category_mask


def category2rgb(category_mask):
    rgb_mask = np.zeros(category_mask.shape[:2] + (3,))
    for i, row in color_dict.iterrows():
        rgb_mask[category_mask == i] = (row["r"], row["g"], row["b"])
    return np.uint8(rgb_mask)

import albumentations as aug

train_augment = aug.Compose(
    [
        aug.Resize(IMAGE_SIZE, IMAGE_SIZE),
        aug.HorizontalFlip(p=0.5),
        aug.VerticalFlip(p=0.5),
        aug.RandomBrightnessContrast(p=0.3),
    ]
)

test_augment = aug.Compose(
    [aug.Resize(IMAGE_SIZE, IMAGE_SIZE), aug.RandomBrightnessContrast(p=0.3)]
)
from torch.utils.data import Dataset, DataLoader
import multiprocessing


class SegmentationDataset(Dataset):
    def __init__(self, df, augmentations=None):
        self.df = df
        self.augmentations = augmentations

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index: int):
        row = self.df.iloc[index]

        image = cv2.imread(row.IMAGES)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        mask = cv2.imread(row.MASKS)
        mask = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        if self.augmentations:
            data = self.augmentations(image=image, mask=mask)
            image = data["image"]
            mask = data["mask"]

        mask = rgb2category(mask)

        image = np.transpose(image, (2, 0, 1)).astype(np.float64)
        mask = np.expand_dims(mask, axis=0)

        image = torch.Tensor(image) / 255.0
        mask = torch.Tensor(mask).long()

        return image, mask


class SegmentationDataModule(pl.LightningDataModule):
    def __init__(self, pd_train, pd_val, pd_test, batch_size=10):
        super().__init__()
        self.pd_train = pd_train
        self.pd_val = pd_val
        self.pd_test = pd_test
        self.batch_size = batch_size

    def setup(self, stage=None):
        self.train_dataset = SegmentationDataset(self.pd_train, train_augment)
        self.val_dataset = SegmentationDataset(self.pd_val, test_augment)
        self.test_dataset = SegmentationDataset(self.pd_test, test_augment)

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size // 2,
            shuffle=False,
            num_workers=1,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size // 2,
            shuffle=False,
            num_workers=1,
        )


def main():
    IMAGE_SIZE = 320
    BATCH_SIZE = 16
    EPOCHS = 5

    # Your existing code here...

    data_module = SegmentationDataModule(
        pd_train, pd_val, pd_test, batch_size=BATCH_SIZE
    )
    data_module.setup()
    image, mask = next(iter(data_module.train_dataloader()))
    print(image.shape, mask.shape)


if __name__ == "__main__":
    # This block will only be executed when the script is run directly
    multiprocessing.freeze_support()  # Necessary for Windows
    main()

from segmentation_models_pytorch import UnetPlusPlus
from segmentation_models_pytorch.losses import DiceLoss
from segmentation_models_pytorch.metrics import (
    get_stats,
    iou_score,
    accuracy,
    precision,
    recall,
    f1_score,
)


class SegmentationModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = UnetPlusPlus(
            encoder_name="timm-regnety_120",
            encoder_weights="imagenet",
            in_channels=3,
            classes=len(CLASSES),
            activation="softmax",
        )
        self.criterion = DiceLoss(mode="multiclass", from_logits=False)

    def forward(self, inputs, targets=None):
        outputs = self.model(inputs)
        if targets is not None:
            loss = self.criterion(outputs, targets)
            tp, fp, fn, tn = get_stats(
                outputs.argmax(dim=1).unsqueeze(1).type(torch.int64),
                targets,
                mode="multiclass",
                num_classes=len(CLASSES),
            )
            metrics = {
                "Accuracy": accuracy(tp, fp, fn, tn, reduction="micro-imagewise"),
                "IoU": iou_score(tp, fp, fn, tn, reduction="micro-imagewise"),
                "Precision": precision(tp, fp, fn, tn, reduction="micro-imagewise"),
                "Recall": recall(tp, fp, fn, tn, reduction="micro-imagewise"),
                "F1score": f1_score(tp, fp, fn, tn, reduction="micro-imagewise"),
            }
            return loss, metrics, outputs
        else:
            return outputs

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=0.0001)

    def training_step(self, batch, batch_idx):
        images, masks = batch

        loss, metrics, outputs = self(images, masks)
        self.log_dict(
            {
                "train/Loss": loss,
                "train/IoU": metrics["IoU"],
                "train/Accuracy": metrics["Accuracy"],
                "train/Precision": metrics["Precision"],
                "train/Recall": metrics["Recall"],
                "train/F1score": metrics["F1score"],
            },
            prog_bar=True,
            logger=True,
            on_step=False,
            on_epoch=True,
        )
        return loss

    def validation_step(self, batch, batch_idx):
        images, masks = batch

        loss, metrics, outputs = self(images, masks)
        self.log_dict(
            {
                "val/Loss": loss,
                "val/IoU": metrics["IoU"],
                "val/Accuracy": metrics["Accuracy"],
                "val/Precision": metrics["Precision"],
                "val/Recall": metrics["Recall"],
                "val/F1score": metrics["F1score"],
            },
            prog_bar=True,
            logger=True,
            on_step=False,
            on_epoch=True,
        )
        return loss

    def test_step(self, batch, batch_idx):
        images, masks = batch

        loss, metrics, outputs = self(images, masks)
        self.log_dict(
            {
                "test/Loss": loss,
                "test/IoU": metrics["IoU"],
                "test/Accuracy": metrics["Accuracy"],
                "test/Precision": metrics["Precision"],
                "test/Recall": metrics["Recall"],
                "test/F1score": metrics["F1score"],
            },
            prog_bar=True,
            logger=True,
            on_step=False,
            on_epoch=True,
        )
        return loss

from torchinfo import summary

model = SegmentationModel()
summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE))

from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import CSVLogger

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val/F1score",
    mode="min",
)

logger = CSVLogger("lightning_logs", name="landcover-classification-log")

early_stopping_callback = EarlyStopping(monitor="val/Accuracy", patience=5)

trainer = pl.Trainer(
    logger=logger,
    log_every_n_steps=31,
    callbacks=[checkpoint_callback, early_stopping_callback],
    max_epochs=EPOCHS,
    accelerator="gpu",
    devices=1,
)
